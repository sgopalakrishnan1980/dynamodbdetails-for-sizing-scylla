# DynamoDB Logs to Excel Processor

This Python script processes DynamoDB metrics log files generated by the Go or Bash collection tools and creates a comprehensive Excel file with organized data.

## Features

- **Automatic Log Discovery**: Scans directories recursively to find log files
- **Excel Output**: Creates organized Excel workbooks with multiple worksheets
- **Data Organization**: Separates Sample Count and P99 Latency data into different worksheets
- **Time Period Separation**: Organizes 3-hour and 7-day data into separate columns
- **Professional Formatting**: Includes headers, styling, and auto-adjusted column widths

## Prerequisites

- Python 3.7 or later
- Required packages (install with `pip install -r requirements.txt`):
  - pandas
  - openpyxl

## Installation

1. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Make the script executable (optional):
   ```bash
   chmod +x process_dynamodb_logs.py
   ```

## Usage

```bash
python process_dynamodb_logs.py <log_directory_path>
```

### Example

```bash
# Process logs from the default output directory
python process_dynamodb_logs.py dynamo_metrics_logs_01012024120000

# Process logs from a custom directory
python process_dynamodb_logs.py /path/to/logs
```

## Output Format

The script creates an Excel file with the following structure:

### File Naming
- Output file: `dynamodb_metrics_YYYYMMDD_HHMMSS.xlsx`

### Worksheet Structure
For each table, two worksheets are created:
1. `{table_name}_sample_count` - Sample Count data
2. `{table_name}_p99_latency` - P99 Latency data

### Column Layout
- **Column A**: Table name (A1) and operation names (A2-A8)
  - A1: Table name
  - A2: GetItem
  - A3: Query
  - A4: Scan
  - A5: PutItem
  - A6: UpdateItem
  - A7: DeleteItem
  - A8: BatchWriteItem

- **Columns B-I**: 3-hour data (20-minute intervals)
  - Headers show timestamps in format: "3hr_YYYY-MM-DD HH:MM"
  - Values are the corresponding metric values

- **Columns I onwards**: 7-day data (24-hour intervals)
  - Headers show timestamps in format: "7day_YYYY-MM-DD HH:MM"
  - Values are the corresponding metric values

### Styling
- Table names are bold and highlighted
- Operation names are bold with light background
- Timestamp headers are bold with gray background
- Column widths are auto-adjusted

## Expected Log File Structure

The script expects log files with the following naming pattern:
```
{table_name}_{operation}_{metric_type}-{period_type}.log
```

### Examples:
- `mytable_GetItem_sample_count-3hr.log`
- `mytable_GetItem_p99_latency-7day.log`
- `users_Query_sample_count-3hr.log`

### Supported Operations:
- GetItem
- Query
- Scan
- PutItem
- UpdateItem
- DeleteItem
- BatchWriteItem

### Supported Metric Types:
- sample_count
- p99_latency

### Supported Period Types:
- 3hr (3-hour data with 20-minute intervals)
- 7day (7-day data with 24-hour intervals)

## Log File Content Format

The script can parse log files with various timestamp formats:

### Supported Timestamp Formats:
1. `2024-01-15 10:30:00: 123.45`
2. `2024-01-15T10:30:00Z: 123.45`
3. Any line containing a timestamp and numeric value

### Example Log Content:
```
================================================
TABLE: mytable
OPERATION: GetItem
METRIC: SampleCount
PERIOD: 3 hours (20-minute intervals)
GENERATED: 2024-01-15 12:00:00
================================================

--- GetItem_SampleCount_20240115_100000to20240115_102000.log ---
2024-01-15 10:00:00: 45.2
2024-01-15 10:20:00: 52.1
2024-01-15 10:40:00: 48.7
```

## Error Handling

The script includes comprehensive error handling:

- **File Not Found**: Gracefully handles missing directories or files
- **Parse Errors**: Continues processing even if individual files can't be parsed
- **Invalid Data**: Skips lines with invalid timestamp or value formats
- **Excel Limits**: Handles Excel worksheet name length limitations

## Troubleshooting

### Common Issues

1. **No log files found**
   - Verify the directory path is correct
   - Check that log files follow the expected naming pattern
   - Ensure files have `.log` extension

2. **Empty Excel file**
   - Check that log files contain valid timestamp-value pairs
   - Verify the log file format matches expected patterns

3. **Missing data**
   - Ensure all expected operations have corresponding log files
   - Check that both 3hr and 7day data files exist

### Debug Information

The script provides detailed output during processing:
- Number of tables found
- Number of files per table
- Processing status for each table
- Final output file location

## Integration with Collection Tools

This script is designed to work with the output from:
- **Go Version**: `./get_dynamodb_metrics`
- **Bash Version**: `./dynamo_metrics_collection.sh`
- **macOS Version**: `./dynamo_metrics_collection_mac.sh`

The script automatically processes the consolidated log files created by these tools.

## Performance Considerations

- Processes files sequentially to avoid memory issues
- Uses efficient file reading and parsing
- Auto-adjusts Excel column widths for readability
- Handles large datasets by processing one table at a time

## License

This script is provided as-is for educational and operational purposes.
